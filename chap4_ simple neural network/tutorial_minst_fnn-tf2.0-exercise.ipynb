{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2151131 朱沙桐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, optimizers, datasets\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    #normalize\n",
    "    x = x/255.0\n",
    "    x_test = x_test/255.0\n",
    "    \n",
    "    return (x, y), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([1, 2, 3, 4], ['a', 'b', 'c', 'd'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel:\n",
    "    def __init__(self):\n",
    "        ####################\n",
    "        '''声明模型对应的参数'''\n",
    "        ####################\n",
    "        self.W1 = tf.Variable(tf.random.normal(\n",
    "            [784, 98]), trainable=True, dtype=tf.float32)\n",
    "        self.b1 = tf.Variable(tf.zeros([98]), trainable=True, dtype=tf.float32)\n",
    "        self.W2 = tf.Variable(tf.random.normal(\n",
    "            [98, 10]), trainable=True, dtype=tf.float32)\n",
    "        self.b2 = tf.Variable(tf.zeros([10]), trainable=True, dtype=tf.float32)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        ####################\n",
    "        '''实现模型函数体，返回未归一化的logits'''\n",
    "        ####################\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        h1 = tf.nn.relu(tf.matmul(x, self.W1) + self.b1)\n",
    "        logits = tf.matmul(h1, self.W2) + self.b2\n",
    "        return logits\n",
    "        \n",
    "model = myModel()\n",
    "\n",
    "optimizer = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels))\n",
    "\n",
    "@tf.function\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "\n",
    "    # compute gradient\n",
    "    trainable_vars = [model.W1, model.W2, model.b1, model.b2]\n",
    "    grads = tape.gradient(loss, trainable_vars)\n",
    "    for g, v in zip(grads, trainable_vars):\n",
    "        v.assign_sub(0.01*g)\n",
    "\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "\n",
    "    # loss and accuracy is scalar tensor\n",
    "    return loss, accuracy\n",
    "\n",
    "@tf.function\n",
    "def test(model, x, y):\n",
    "    logits = model(x)\n",
    "    loss = compute_loss(logits, y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 91.84845 ; accuracy 0.10211667\n",
      "epoch 1 : loss 83.035614 ; accuracy 0.10275\n",
      "epoch 2 : loss 76.28547 ; accuracy 0.10446667\n",
      "epoch 3 : loss 71.08835 ; accuracy 0.10538334\n",
      "epoch 4 : loss 66.96749 ; accuracy 0.10718333\n",
      "epoch 5 : loss 63.6277 ; accuracy 0.109066665\n",
      "epoch 6 : loss 60.84216 ; accuracy 0.1117\n",
      "epoch 7 : loss 58.489086 ; accuracy 0.11375\n",
      "epoch 8 : loss 56.47683 ; accuracy 0.11655\n",
      "epoch 9 : loss 54.735508 ; accuracy 0.1195\n",
      "epoch 10 : loss 53.21015 ; accuracy 0.12261666\n",
      "epoch 11 : loss 51.85543 ; accuracy 0.12598333\n",
      "epoch 12 : loss 50.639515 ; accuracy 0.13011667\n",
      "epoch 13 : loss 49.53517 ; accuracy 0.13463333\n",
      "epoch 14 : loss 48.52185 ; accuracy 0.13966666\n",
      "epoch 15 : loss 47.583923 ; accuracy 0.14446667\n",
      "epoch 16 : loss 46.707985 ; accuracy 0.14996667\n",
      "epoch 17 : loss 45.884857 ; accuracy 0.15493333\n",
      "epoch 18 : loss 45.106407 ; accuracy 0.16051666\n",
      "epoch 19 : loss 44.3656 ; accuracy 0.16578333\n",
      "epoch 20 : loss 43.656944 ; accuracy 0.17101666\n",
      "epoch 21 : loss 42.97686 ; accuracy 0.17621666\n",
      "epoch 22 : loss 42.322506 ; accuracy 0.18095\n",
      "epoch 23 : loss 41.69117 ; accuracy 0.1853\n",
      "epoch 24 : loss 41.08102 ; accuracy 0.19001667\n",
      "epoch 25 : loss 40.490753 ; accuracy 0.19508334\n",
      "epoch 26 : loss 39.91938 ; accuracy 0.20006667\n",
      "epoch 27 : loss 39.3659 ; accuracy 0.20508334\n",
      "epoch 28 : loss 38.82905 ; accuracy 0.20988333\n",
      "epoch 29 : loss 38.307625 ; accuracy 0.21435\n",
      "epoch 30 : loss 37.800724 ; accuracy 0.21878333\n",
      "epoch 31 : loss 37.307682 ; accuracy 0.22298333\n",
      "epoch 32 : loss 36.828014 ; accuracy 0.22723334\n",
      "epoch 33 : loss 36.3611 ; accuracy 0.23165\n",
      "epoch 34 : loss 35.906082 ; accuracy 0.23535\n",
      "epoch 35 : loss 35.462414 ; accuracy 0.23915\n",
      "epoch 36 : loss 35.029476 ; accuracy 0.24336667\n",
      "epoch 37 : loss 34.60715 ; accuracy 0.24733333\n",
      "epoch 38 : loss 34.19507 ; accuracy 0.2514\n",
      "epoch 39 : loss 33.792797 ; accuracy 0.25471666\n",
      "epoch 40 : loss 33.39978 ; accuracy 0.25838333\n",
      "epoch 41 : loss 33.015636 ; accuracy 0.26205\n",
      "epoch 42 : loss 32.64019 ; accuracy 0.26586667\n",
      "epoch 43 : loss 32.273468 ; accuracy 0.26978335\n",
      "epoch 44 : loss 31.915209 ; accuracy 0.27335\n",
      "epoch 45 : loss 31.565165 ; accuracy 0.27695\n",
      "epoch 46 : loss 31.223015 ; accuracy 0.28051665\n",
      "epoch 47 : loss 30.888329 ; accuracy 0.28411666\n",
      "epoch 48 : loss 30.560787 ; accuracy 0.2875\n",
      "epoch 49 : loss 30.24013 ; accuracy 0.29065\n",
      "epoch 50 : loss 29.92616 ; accuracy 0.29398334\n",
      "epoch 51 : loss 29.618769 ; accuracy 0.29725\n",
      "epoch 52 : loss 29.317783 ; accuracy 0.3006\n",
      "epoch 53 : loss 29.023146 ; accuracy 0.3041\n",
      "epoch 54 : loss 28.734694 ; accuracy 0.3076\n",
      "epoch 55 : loss 28.452133 ; accuracy 0.31091666\n",
      "epoch 56 : loss 28.175238 ; accuracy 0.31386667\n",
      "epoch 57 : loss 27.903917 ; accuracy 0.31688333\n",
      "epoch 58 : loss 27.638035 ; accuracy 0.31983334\n",
      "epoch 59 : loss 27.377424 ; accuracy 0.3226\n",
      "epoch 60 : loss 27.121916 ; accuracy 0.32521668\n",
      "epoch 61 : loss 26.8714 ; accuracy 0.32783332\n",
      "epoch 62 : loss 26.625708 ; accuracy 0.33045\n",
      "epoch 63 : loss 26.384695 ; accuracy 0.33346668\n",
      "epoch 64 : loss 26.148241 ; accuracy 0.33675\n",
      "epoch 65 : loss 25.916256 ; accuracy 0.3392\n",
      "epoch 66 : loss 25.688597 ; accuracy 0.34206668\n",
      "epoch 67 : loss 25.465088 ; accuracy 0.34515\n",
      "epoch 68 : loss 25.245726 ; accuracy 0.34793332\n",
      "epoch 69 : loss 25.030355 ; accuracy 0.35033333\n",
      "epoch 70 : loss 24.818851 ; accuracy 0.35283333\n",
      "epoch 71 : loss 24.611145 ; accuracy 0.35526666\n",
      "epoch 72 : loss 24.40716 ; accuracy 0.35775\n",
      "epoch 73 : loss 24.206705 ; accuracy 0.35991666\n",
      "epoch 74 : loss 24.009642 ; accuracy 0.36236668\n",
      "epoch 75 : loss 23.815872 ; accuracy 0.36483333\n",
      "epoch 76 : loss 23.625343 ; accuracy 0.36731666\n",
      "epoch 77 : loss 23.437994 ; accuracy 0.36988333\n",
      "epoch 78 : loss 23.253729 ; accuracy 0.3723\n",
      "epoch 79 : loss 23.072508 ; accuracy 0.37503332\n",
      "epoch 80 : loss 22.89429 ; accuracy 0.37733334\n",
      "epoch 81 : loss 22.719017 ; accuracy 0.37971666\n",
      "epoch 82 : loss 22.546566 ; accuracy 0.38193333\n",
      "epoch 83 : loss 22.376892 ; accuracy 0.3848\n",
      "epoch 84 : loss 22.209974 ; accuracy 0.38733333\n",
      "epoch 85 : loss 22.045706 ; accuracy 0.38938335\n",
      "epoch 86 : loss 21.883995 ; accuracy 0.3919\n",
      "epoch 87 : loss 21.724842 ; accuracy 0.39403334\n",
      "epoch 88 : loss 21.568184 ; accuracy 0.39623332\n",
      "epoch 89 : loss 21.413996 ; accuracy 0.39863333\n",
      "epoch 90 : loss 21.262203 ; accuracy 0.40085\n",
      "epoch 91 : loss 21.112741 ; accuracy 0.40313333\n",
      "epoch 92 : loss 20.965567 ; accuracy 0.40536666\n",
      "epoch 93 : loss 20.82062 ; accuracy 0.40765\n",
      "epoch 94 : loss 20.677837 ; accuracy 0.40978333\n",
      "epoch 95 : loss 20.537205 ; accuracy 0.41215\n",
      "epoch 96 : loss 20.398687 ; accuracy 0.41396666\n",
      "epoch 97 : loss 20.26222 ; accuracy 0.416\n",
      "epoch 98 : loss 20.127733 ; accuracy 0.41776666\n",
      "epoch 99 : loss 19.995176 ; accuracy 0.41961667\n",
      "epoch 100 : loss 19.864523 ; accuracy 0.42163333\n",
      "epoch 101 : loss 19.735716 ; accuracy 0.42371666\n",
      "epoch 102 : loss 19.608719 ; accuracy 0.42543334\n",
      "epoch 103 : loss 19.483496 ; accuracy 0.42735\n",
      "epoch 104 : loss 19.359995 ; accuracy 0.42935\n",
      "epoch 105 : loss 19.23816 ; accuracy 0.4312\n",
      "epoch 106 : loss 19.117962 ; accuracy 0.43288332\n",
      "epoch 107 : loss 18.99934 ; accuracy 0.43456668\n",
      "epoch 108 : loss 18.882284 ; accuracy 0.43658334\n",
      "epoch 109 : loss 18.766779 ; accuracy 0.43818334\n",
      "epoch 110 : loss 18.65277 ; accuracy 0.43985\n",
      "epoch 111 : loss 18.540224 ; accuracy 0.44145\n",
      "epoch 112 : loss 18.429108 ; accuracy 0.443\n",
      "epoch 113 : loss 18.31938 ; accuracy 0.44488335\n",
      "epoch 114 : loss 18.211008 ; accuracy 0.44645\n",
      "epoch 115 : loss 18.103971 ; accuracy 0.44821668\n",
      "epoch 116 : loss 17.998241 ; accuracy 0.44986665\n",
      "epoch 117 : loss 17.893795 ; accuracy 0.45148334\n",
      "epoch 118 : loss 17.790638 ; accuracy 0.45341668\n",
      "epoch 119 : loss 17.688725 ; accuracy 0.45505\n",
      "epoch 120 : loss 17.588062 ; accuracy 0.45635\n",
      "epoch 121 : loss 17.488634 ; accuracy 0.45796666\n",
      "epoch 122 : loss 17.390385 ; accuracy 0.45948333\n",
      "epoch 123 : loss 17.2933 ; accuracy 0.46088332\n",
      "epoch 124 : loss 17.19737 ; accuracy 0.46258333\n",
      "epoch 125 : loss 17.102573 ; accuracy 0.46418333\n",
      "epoch 126 : loss 17.008904 ; accuracy 0.4657\n",
      "epoch 127 : loss 16.916328 ; accuracy 0.46716666\n",
      "epoch 128 : loss 16.824831 ; accuracy 0.46841666\n",
      "epoch 129 : loss 16.734362 ; accuracy 0.46986666\n",
      "epoch 130 : loss 16.644896 ; accuracy 0.47143334\n",
      "epoch 131 : loss 16.556427 ; accuracy 0.47278333\n",
      "epoch 132 : loss 16.468945 ; accuracy 0.4744\n",
      "epoch 133 : loss 16.382433 ; accuracy 0.47573334\n",
      "epoch 134 : loss 16.296867 ; accuracy 0.47715\n",
      "epoch 135 : loss 16.21223 ; accuracy 0.47881666\n",
      "epoch 136 : loss 16.1285 ; accuracy 0.4803\n",
      "epoch 137 : loss 16.045662 ; accuracy 0.48135\n",
      "epoch 138 : loss 15.963694 ; accuracy 0.4825\n",
      "epoch 139 : loss 15.882578 ; accuracy 0.4838\n",
      "epoch 140 : loss 15.802313 ; accuracy 0.48525\n",
      "epoch 141 : loss 15.7228565 ; accuracy 0.4864\n",
      "epoch 142 : loss 15.644205 ; accuracy 0.48756668\n",
      "epoch 143 : loss 15.566352 ; accuracy 0.48868334\n",
      "epoch 144 : loss 15.489294 ; accuracy 0.4898\n",
      "epoch 145 : loss 15.413021 ; accuracy 0.49096668\n",
      "epoch 146 : loss 15.337527 ; accuracy 0.49223334\n",
      "epoch 147 : loss 15.262782 ; accuracy 0.49363333\n",
      "epoch 148 : loss 15.188786 ; accuracy 0.49491668\n",
      "epoch 149 : loss 15.11554 ; accuracy 0.49608332\n",
      "epoch 150 : loss 15.043052 ; accuracy 0.49721667\n",
      "epoch 151 : loss 14.971311 ; accuracy 0.49853334\n",
      "epoch 152 : loss 14.900288 ; accuracy 0.49993333\n",
      "epoch 153 : loss 14.829991 ; accuracy 0.50121665\n",
      "epoch 154 : loss 14.760417 ; accuracy 0.50238335\n",
      "epoch 155 : loss 14.6915455 ; accuracy 0.5037\n",
      "epoch 156 : loss 14.623364 ; accuracy 0.50493336\n",
      "epoch 157 : loss 14.555885 ; accuracy 0.5063\n",
      "epoch 158 : loss 14.489089 ; accuracy 0.50771666\n",
      "epoch 159 : loss 14.422958 ; accuracy 0.50878334\n",
      "epoch 160 : loss 14.357488 ; accuracy 0.5100167\n",
      "epoch 161 : loss 14.292668 ; accuracy 0.51101667\n",
      "epoch 162 : loss 14.228502 ; accuracy 0.5122167\n",
      "epoch 163 : loss 14.164981 ; accuracy 0.5136167\n",
      "epoch 164 : loss 14.102102 ; accuracy 0.5148\n",
      "epoch 165 : loss 14.039841 ; accuracy 0.5159\n",
      "epoch 166 : loss 13.978196 ; accuracy 0.51711667\n",
      "epoch 167 : loss 13.91716 ; accuracy 0.51825\n",
      "epoch 168 : loss 13.856725 ; accuracy 0.5194\n",
      "epoch 169 : loss 13.796885 ; accuracy 0.52055\n",
      "epoch 170 : loss 13.737618 ; accuracy 0.52171665\n",
      "epoch 171 : loss 13.678931 ; accuracy 0.52271664\n",
      "epoch 172 : loss 13.620813 ; accuracy 0.52375\n",
      "epoch 173 : loss 13.56325 ; accuracy 0.52501667\n",
      "epoch 174 : loss 13.506223 ; accuracy 0.52625\n",
      "epoch 175 : loss 13.449725 ; accuracy 0.5272167\n",
      "epoch 176 : loss 13.393753 ; accuracy 0.52856666\n",
      "epoch 177 : loss 13.338308 ; accuracy 0.52955\n",
      "epoch 178 : loss 13.283372 ; accuracy 0.5308167\n",
      "epoch 179 : loss 13.228949 ; accuracy 0.532\n",
      "epoch 180 : loss 13.175035 ; accuracy 0.53291667\n",
      "epoch 181 : loss 13.121625 ; accuracy 0.5341833\n",
      "epoch 182 : loss 13.068705 ; accuracy 0.53543335\n",
      "epoch 183 : loss 13.016265 ; accuracy 0.5365667\n",
      "epoch 184 : loss 12.964307 ; accuracy 0.5377333\n",
      "epoch 185 : loss 12.912816 ; accuracy 0.5387\n",
      "epoch 186 : loss 12.861788 ; accuracy 0.53965\n",
      "epoch 187 : loss 12.811221 ; accuracy 0.54045\n",
      "epoch 188 : loss 12.761096 ; accuracy 0.54135\n",
      "epoch 189 : loss 12.711409 ; accuracy 0.54218334\n",
      "epoch 190 : loss 12.662158 ; accuracy 0.5430167\n",
      "epoch 191 : loss 12.613336 ; accuracy 0.54368335\n",
      "epoch 192 : loss 12.564939 ; accuracy 0.5444\n",
      "epoch 193 : loss 12.516965 ; accuracy 0.5454\n",
      "epoch 194 : loss 12.469392 ; accuracy 0.54641664\n",
      "epoch 195 : loss 12.422221 ; accuracy 0.5473667\n",
      "epoch 196 : loss 12.375462 ; accuracy 0.5484167\n",
      "epoch 197 : loss 12.3291025 ; accuracy 0.54915\n",
      "epoch 198 : loss 12.283144 ; accuracy 0.55048335\n",
      "epoch 199 : loss 12.237581 ; accuracy 0.55135\n",
      "epoch 200 : loss 12.19241 ; accuracy 0.5523\n",
      "epoch 201 : loss 12.147618 ; accuracy 0.55308336\n",
      "epoch 202 : loss 12.103199 ; accuracy 0.55425\n",
      "epoch 203 : loss 12.059152 ; accuracy 0.5552\n",
      "epoch 204 : loss 12.015479 ; accuracy 0.5560667\n",
      "epoch 205 : loss 11.972173 ; accuracy 0.55705\n",
      "epoch 206 : loss 11.929239 ; accuracy 0.5578833\n",
      "epoch 207 : loss 11.886669 ; accuracy 0.5587\n",
      "epoch 208 : loss 11.84445 ; accuracy 0.55953336\n",
      "epoch 209 : loss 11.802586 ; accuracy 0.56021667\n",
      "epoch 210 : loss 11.761075 ; accuracy 0.56098336\n",
      "epoch 211 : loss 11.719891 ; accuracy 0.5617833\n",
      "epoch 212 : loss 11.679044 ; accuracy 0.56255\n",
      "epoch 213 : loss 11.638516 ; accuracy 0.5632667\n",
      "epoch 214 : loss 11.598307 ; accuracy 0.5640333\n",
      "epoch 215 : loss 11.558418 ; accuracy 0.56521666\n",
      "epoch 216 : loss 11.518845 ; accuracy 0.56598336\n",
      "epoch 217 : loss 11.479586 ; accuracy 0.56695\n",
      "epoch 218 : loss 11.440629 ; accuracy 0.5678167\n",
      "epoch 219 : loss 11.401978 ; accuracy 0.5687\n",
      "epoch 220 : loss 11.363624 ; accuracy 0.5696\n",
      "epoch 221 : loss 11.325566 ; accuracy 0.57045\n",
      "epoch 222 : loss 11.287803 ; accuracy 0.57133335\n",
      "epoch 223 : loss 11.250336 ; accuracy 0.57208335\n",
      "epoch 224 : loss 11.213163 ; accuracy 0.5729\n",
      "epoch 225 : loss 11.176282 ; accuracy 0.5736333\n",
      "epoch 226 : loss 11.139694 ; accuracy 0.5746667\n",
      "epoch 227 : loss 11.103393 ; accuracy 0.57535\n",
      "epoch 228 : loss 11.067372 ; accuracy 0.57615\n",
      "epoch 229 : loss 11.031623 ; accuracy 0.57705\n",
      "epoch 230 : loss 10.996152 ; accuracy 0.57771665\n",
      "epoch 231 : loss 10.960957 ; accuracy 0.5786333\n",
      "epoch 232 : loss 10.926021 ; accuracy 0.57948333\n",
      "epoch 233 : loss 10.891341 ; accuracy 0.5804\n",
      "epoch 234 : loss 10.856925 ; accuracy 0.58126664\n",
      "epoch 235 : loss 10.822769 ; accuracy 0.582\n",
      "epoch 236 : loss 10.78887 ; accuracy 0.58295\n",
      "epoch 237 : loss 10.755231 ; accuracy 0.58383334\n",
      "epoch 238 : loss 10.721852 ; accuracy 0.5846\n",
      "epoch 239 : loss 10.688723 ; accuracy 0.5854333\n",
      "epoch 240 : loss 10.655829 ; accuracy 0.5862333\n",
      "epoch 241 : loss 10.623178 ; accuracy 0.58708334\n",
      "epoch 242 : loss 10.590771 ; accuracy 0.5877333\n",
      "epoch 243 : loss 10.5586 ; accuracy 0.5885\n",
      "epoch 244 : loss 10.526656 ; accuracy 0.5890167\n",
      "epoch 245 : loss 10.49494 ; accuracy 0.58966666\n",
      "epoch 246 : loss 10.463452 ; accuracy 0.59028333\n",
      "epoch 247 : loss 10.432198 ; accuracy 0.59113336\n",
      "epoch 248 : loss 10.401165 ; accuracy 0.59188336\n",
      "epoch 249 : loss 10.370354 ; accuracy 0.59258336\n",
      "epoch 250 : loss 10.339761 ; accuracy 0.5934167\n",
      "epoch 251 : loss 10.3093815 ; accuracy 0.59425\n",
      "epoch 252 : loss 10.279219 ; accuracy 0.59496665\n",
      "epoch 253 : loss 10.249272 ; accuracy 0.59566665\n",
      "epoch 254 : loss 10.219533 ; accuracy 0.5962\n",
      "epoch 255 : loss 10.190003 ; accuracy 0.59706664\n",
      "epoch 256 : loss 10.160673 ; accuracy 0.59776664\n",
      "epoch 257 : loss 10.131548 ; accuracy 0.5984333\n",
      "epoch 258 : loss 10.102624 ; accuracy 0.5991\n",
      "epoch 259 : loss 10.073902 ; accuracy 0.59966666\n",
      "epoch 260 : loss 10.045376 ; accuracy 0.60036665\n",
      "epoch 261 : loss 10.017062 ; accuracy 0.6012833\n",
      "epoch 262 : loss 9.988958 ; accuracy 0.6019667\n",
      "epoch 263 : loss 9.961059 ; accuracy 0.60263336\n",
      "epoch 264 : loss 9.9333515 ; accuracy 0.6031\n",
      "epoch 265 : loss 9.905837 ; accuracy 0.6038167\n",
      "epoch 266 : loss 9.878524 ; accuracy 0.6045833\n",
      "epoch 267 : loss 9.8514 ; accuracy 0.60511667\n",
      "epoch 268 : loss 9.824463 ; accuracy 0.6057\n",
      "epoch 269 : loss 9.79771 ; accuracy 0.60623336\n",
      "epoch 270 : loss 9.771139 ; accuracy 0.6070667\n",
      "epoch 271 : loss 9.744743 ; accuracy 0.60765\n",
      "epoch 272 : loss 9.718527 ; accuracy 0.60833335\n",
      "epoch 273 : loss 9.692487 ; accuracy 0.60903335\n",
      "epoch 274 : loss 9.66662 ; accuracy 0.60975\n",
      "epoch 275 : loss 9.640922 ; accuracy 0.6103333\n",
      "epoch 276 : loss 9.615394 ; accuracy 0.61073333\n",
      "epoch 277 : loss 9.590037 ; accuracy 0.61145\n",
      "epoch 278 : loss 9.564855 ; accuracy 0.61215\n",
      "epoch 279 : loss 9.539839 ; accuracy 0.6128167\n",
      "epoch 280 : loss 9.514983 ; accuracy 0.6135167\n",
      "epoch 281 : loss 9.490288 ; accuracy 0.6141667\n",
      "epoch 282 : loss 9.4657545 ; accuracy 0.61481667\n",
      "epoch 283 : loss 9.441377 ; accuracy 0.61545\n",
      "epoch 284 : loss 9.417156 ; accuracy 0.61586666\n",
      "epoch 285 : loss 9.393094 ; accuracy 0.61646664\n",
      "epoch 286 : loss 9.369183 ; accuracy 0.61723334\n",
      "epoch 287 : loss 9.345425 ; accuracy 0.61785\n",
      "epoch 288 : loss 9.321816 ; accuracy 0.61838335\n",
      "epoch 289 : loss 9.298361 ; accuracy 0.6189333\n",
      "epoch 290 : loss 9.275055 ; accuracy 0.61965\n",
      "epoch 291 : loss 9.251898 ; accuracy 0.6203\n",
      "epoch 292 : loss 9.228889 ; accuracy 0.6207\n",
      "epoch 293 : loss 9.206027 ; accuracy 0.62128335\n",
      "epoch 294 : loss 9.183312 ; accuracy 0.62175\n",
      "epoch 295 : loss 9.16074 ; accuracy 0.6224333\n",
      "epoch 296 : loss 9.138312 ; accuracy 0.62295\n",
      "epoch 297 : loss 9.116016 ; accuracy 0.62365\n",
      "epoch 298 : loss 9.093859 ; accuracy 0.6242\n",
      "epoch 299 : loss 9.071829 ; accuracy 0.62488335\n",
      "epoch 300 : loss 9.049937 ; accuracy 0.62553334\n",
      "epoch 301 : loss 9.028177 ; accuracy 0.62595\n",
      "epoch 302 : loss 9.006552 ; accuracy 0.62656665\n",
      "epoch 303 : loss 8.985056 ; accuracy 0.6271833\n",
      "epoch 304 : loss 8.963694 ; accuracy 0.62768334\n",
      "epoch 305 : loss 8.942458 ; accuracy 0.6281833\n",
      "epoch 306 : loss 8.92135 ; accuracy 0.62901664\n",
      "epoch 307 : loss 8.900369 ; accuracy 0.62946665\n",
      "epoch 308 : loss 8.8795185 ; accuracy 0.6299833\n",
      "epoch 309 : loss 8.858796 ; accuracy 0.63055\n",
      "epoch 310 : loss 8.838194 ; accuracy 0.63095\n",
      "epoch 311 : loss 8.817706 ; accuracy 0.6316\n",
      "epoch 312 : loss 8.797338 ; accuracy 0.6321167\n",
      "epoch 313 : loss 8.777087 ; accuracy 0.6327\n",
      "epoch 314 : loss 8.756963 ; accuracy 0.6331667\n",
      "epoch 315 : loss 8.736959 ; accuracy 0.63376665\n",
      "epoch 316 : loss 8.717079 ; accuracy 0.63425\n",
      "epoch 317 : loss 8.697323 ; accuracy 0.6346833\n",
      "epoch 318 : loss 8.677685 ; accuracy 0.6352\n",
      "epoch 319 : loss 8.65816 ; accuracy 0.6358\n",
      "epoch 320 : loss 8.638751 ; accuracy 0.63638335\n",
      "epoch 321 : loss 8.619462 ; accuracy 0.6368\n",
      "epoch 322 : loss 8.600289 ; accuracy 0.6374\n",
      "epoch 323 : loss 8.581241 ; accuracy 0.63781667\n",
      "epoch 324 : loss 8.562307 ; accuracy 0.6383833\n",
      "epoch 325 : loss 8.543486 ; accuracy 0.6389\n",
      "epoch 326 : loss 8.524774 ; accuracy 0.6393667\n",
      "epoch 327 : loss 8.506173 ; accuracy 0.63996667\n",
      "epoch 328 : loss 8.487679 ; accuracy 0.64061666\n",
      "epoch 329 : loss 8.469285 ; accuracy 0.6412333\n",
      "epoch 330 : loss 8.451 ; accuracy 0.6418\n",
      "epoch 331 : loss 8.432818 ; accuracy 0.64236665\n",
      "epoch 332 : loss 8.414739 ; accuracy 0.6429\n",
      "epoch 333 : loss 8.396764 ; accuracy 0.6433167\n",
      "epoch 334 : loss 8.37889 ; accuracy 0.6436\n",
      "epoch 335 : loss 8.36112 ; accuracy 0.64405\n",
      "epoch 336 : loss 8.34345 ; accuracy 0.6444\n",
      "epoch 337 : loss 8.325877 ; accuracy 0.64493334\n",
      "epoch 338 : loss 8.308399 ; accuracy 0.6454167\n",
      "epoch 339 : loss 8.2910185 ; accuracy 0.6458833\n",
      "epoch 340 : loss 8.273732 ; accuracy 0.6462833\n",
      "epoch 341 : loss 8.256539 ; accuracy 0.64676666\n",
      "epoch 342 : loss 8.239439 ; accuracy 0.6472167\n",
      "epoch 343 : loss 8.222428 ; accuracy 0.6475833\n",
      "epoch 344 : loss 8.205515 ; accuracy 0.6479667\n",
      "epoch 345 : loss 8.188692 ; accuracy 0.6485\n",
      "epoch 346 : loss 8.171957 ; accuracy 0.6491167\n",
      "epoch 347 : loss 8.155319 ; accuracy 0.6495\n",
      "epoch 348 : loss 8.138765 ; accuracy 0.6501\n",
      "epoch 349 : loss 8.122302 ; accuracy 0.6504833\n",
      "epoch 350 : loss 8.105923 ; accuracy 0.6508167\n",
      "epoch 351 : loss 8.089632 ; accuracy 0.6512167\n",
      "epoch 352 : loss 8.073428 ; accuracy 0.65185\n",
      "epoch 353 : loss 8.057309 ; accuracy 0.6524\n",
      "epoch 354 : loss 8.04127 ; accuracy 0.65283334\n",
      "epoch 355 : loss 8.02532 ; accuracy 0.6533333\n",
      "epoch 356 : loss 8.0094595 ; accuracy 0.65386665\n",
      "epoch 357 : loss 7.993687 ; accuracy 0.6541833\n",
      "epoch 358 : loss 7.977995 ; accuracy 0.6546\n",
      "epoch 359 : loss 7.9623857 ; accuracy 0.65505\n",
      "epoch 360 : loss 7.9468603 ; accuracy 0.65561664\n",
      "epoch 361 : loss 7.9314165 ; accuracy 0.65596664\n",
      "epoch 362 : loss 7.916055 ; accuracy 0.65653336\n",
      "epoch 363 : loss 7.9007707 ; accuracy 0.65676665\n",
      "epoch 364 : loss 7.885565 ; accuracy 0.65721667\n",
      "epoch 365 : loss 7.8704367 ; accuracy 0.6576\n",
      "epoch 366 : loss 7.8553886 ; accuracy 0.65788335\n",
      "epoch 367 : loss 7.840419 ; accuracy 0.65815\n",
      "epoch 368 : loss 7.825526 ; accuracy 0.65865\n",
      "epoch 369 : loss 7.8107114 ; accuracy 0.65898335\n",
      "epoch 370 : loss 7.7959676 ; accuracy 0.6594333\n",
      "epoch 371 : loss 7.781299 ; accuracy 0.6598333\n",
      "epoch 372 : loss 7.7667093 ; accuracy 0.6602333\n",
      "epoch 373 : loss 7.7521935 ; accuracy 0.66065\n",
      "epoch 374 : loss 7.737749 ; accuracy 0.66105\n",
      "epoch 375 : loss 7.72338 ; accuracy 0.6614\n",
      "epoch 376 : loss 7.7090864 ; accuracy 0.6618\n",
      "epoch 377 : loss 7.6948643 ; accuracy 0.66213334\n",
      "epoch 378 : loss 7.6807127 ; accuracy 0.66253334\n",
      "epoch 379 : loss 7.6666303 ; accuracy 0.6630167\n",
      "epoch 380 : loss 7.652619 ; accuracy 0.6634833\n",
      "epoch 381 : loss 7.6386766 ; accuracy 0.66386664\n",
      "epoch 382 : loss 7.624804 ; accuracy 0.66445\n",
      "epoch 383 : loss 7.6110024 ; accuracy 0.66473335\n",
      "epoch 384 : loss 7.5972667 ; accuracy 0.66508335\n",
      "epoch 385 : loss 7.5835953 ; accuracy 0.6655333\n",
      "epoch 386 : loss 7.569994 ; accuracy 0.66573334\n",
      "epoch 387 : loss 7.556458 ; accuracy 0.66605\n",
      "epoch 388 : loss 7.5429864 ; accuracy 0.6664\n",
      "epoch 389 : loss 7.529575 ; accuracy 0.66688335\n",
      "epoch 390 : loss 7.5162287 ; accuracy 0.66723335\n",
      "epoch 391 : loss 7.5029464 ; accuracy 0.66775\n",
      "epoch 392 : loss 7.4897304 ; accuracy 0.66803336\n",
      "epoch 393 : loss 7.47658 ; accuracy 0.66838336\n",
      "epoch 394 : loss 7.463495 ; accuracy 0.66875\n",
      "epoch 395 : loss 7.45047 ; accuracy 0.66908336\n",
      "epoch 396 : loss 7.4375052 ; accuracy 0.6694667\n",
      "epoch 397 : loss 7.4246054 ; accuracy 0.6698667\n",
      "epoch 398 : loss 7.4117665 ; accuracy 0.6702667\n",
      "epoch 399 : loss 7.398989 ; accuracy 0.6706667\n",
      "epoch 400 : loss 7.386268 ; accuracy 0.67105\n",
      "epoch 401 : loss 7.3736076 ; accuracy 0.6713833\n",
      "epoch 402 : loss 7.3610077 ; accuracy 0.6717333\n",
      "epoch 403 : loss 7.348463 ; accuracy 0.6721333\n",
      "epoch 404 : loss 7.3359804 ; accuracy 0.6724667\n",
      "epoch 405 : loss 7.323554 ; accuracy 0.67298335\n",
      "epoch 406 : loss 7.3111825 ; accuracy 0.67335\n",
      "epoch 407 : loss 7.2988644 ; accuracy 0.67371666\n",
      "epoch 408 : loss 7.286599 ; accuracy 0.6742\n",
      "epoch 409 : loss 7.2743883 ; accuracy 0.67438334\n",
      "epoch 410 : loss 7.2622356 ; accuracy 0.6746333\n",
      "epoch 411 : loss 7.2501388 ; accuracy 0.6749\n",
      "epoch 412 : loss 7.238098 ; accuracy 0.6752333\n",
      "epoch 413 : loss 7.226113 ; accuracy 0.6755667\n",
      "epoch 414 : loss 7.2141795 ; accuracy 0.6759\n",
      "epoch 415 : loss 7.202299 ; accuracy 0.6763\n",
      "epoch 416 : loss 7.190475 ; accuracy 0.6766667\n",
      "epoch 417 : loss 7.1787047 ; accuracy 0.67688334\n",
      "epoch 418 : loss 7.166989 ; accuracy 0.6773667\n",
      "epoch 419 : loss 7.1553297 ; accuracy 0.67761666\n",
      "epoch 420 : loss 7.143722 ; accuracy 0.67793334\n",
      "epoch 421 : loss 7.1321654 ; accuracy 0.67833334\n",
      "epoch 422 : loss 7.120659 ; accuracy 0.67865\n",
      "epoch 423 : loss 7.109202 ; accuracy 0.6789\n",
      "epoch 424 : loss 7.0977974 ; accuracy 0.67915\n",
      "epoch 425 : loss 7.086444 ; accuracy 0.6795167\n",
      "epoch 426 : loss 7.075139 ; accuracy 0.6800167\n",
      "epoch 427 : loss 7.063885 ; accuracy 0.68048334\n",
      "epoch 428 : loss 7.052679 ; accuracy 0.6807167\n",
      "epoch 429 : loss 7.0415196 ; accuracy 0.6810667\n",
      "epoch 430 : loss 7.0304093 ; accuracy 0.68146664\n",
      "epoch 431 : loss 7.0193467 ; accuracy 0.6817333\n",
      "epoch 432 : loss 7.0083313 ; accuracy 0.6820833\n",
      "epoch 433 : loss 6.9973607 ; accuracy 0.6824667\n",
      "epoch 434 : loss 6.9864373 ; accuracy 0.6828333\n",
      "epoch 435 : loss 6.975559 ; accuracy 0.6831333\n",
      "epoch 436 : loss 6.9647284 ; accuracy 0.6834\n",
      "epoch 437 : loss 6.953947 ; accuracy 0.6835833\n",
      "epoch 438 : loss 6.94321 ; accuracy 0.68405\n",
      "epoch 439 : loss 6.932518 ; accuracy 0.68441665\n",
      "epoch 440 : loss 6.921873 ; accuracy 0.68478334\n",
      "epoch 441 : loss 6.911272 ; accuracy 0.6850833\n",
      "epoch 442 : loss 6.900717 ; accuracy 0.6854\n",
      "epoch 443 : loss 6.8902082 ; accuracy 0.6856833\n",
      "epoch 444 : loss 6.879745 ; accuracy 0.6861167\n",
      "epoch 445 : loss 6.8693233 ; accuracy 0.68653333\n",
      "epoch 446 : loss 6.858948 ; accuracy 0.68686664\n",
      "epoch 447 : loss 6.8486176 ; accuracy 0.6873\n",
      "epoch 448 : loss 6.83833 ; accuracy 0.68768334\n",
      "epoch 449 : loss 6.828084 ; accuracy 0.68805\n",
      "epoch 450 : loss 6.8178787 ; accuracy 0.6881667\n",
      "epoch 451 : loss 6.8077173 ; accuracy 0.6885333\n",
      "epoch 452 : loss 6.797597 ; accuracy 0.68881667\n",
      "epoch 453 : loss 6.7875175 ; accuracy 0.6892\n",
      "epoch 454 : loss 6.7774825 ; accuracy 0.6894\n",
      "epoch 455 : loss 6.7674875 ; accuracy 0.68981665\n",
      "epoch 456 : loss 6.757531 ; accuracy 0.6900167\n",
      "epoch 457 : loss 6.7476163 ; accuracy 0.69026667\n",
      "epoch 458 : loss 6.7377405 ; accuracy 0.6905\n",
      "epoch 459 : loss 6.727905 ; accuracy 0.69081664\n",
      "epoch 460 : loss 6.7181087 ; accuracy 0.69121665\n",
      "epoch 461 : loss 6.70835 ; accuracy 0.69161665\n",
      "epoch 462 : loss 6.6986294 ; accuracy 0.6919\n",
      "epoch 463 : loss 6.688948 ; accuracy 0.69215\n",
      "epoch 464 : loss 6.6793056 ; accuracy 0.69241667\n",
      "epoch 465 : loss 6.6697 ; accuracy 0.69278336\n",
      "epoch 466 : loss 6.660132 ; accuracy 0.69306666\n",
      "epoch 467 : loss 6.6506042 ; accuracy 0.69331664\n",
      "epoch 468 : loss 6.6411157 ; accuracy 0.6936833\n",
      "epoch 469 : loss 6.6316624 ; accuracy 0.69395\n",
      "epoch 470 : loss 6.6222453 ; accuracy 0.6942667\n",
      "epoch 471 : loss 6.6128654 ; accuracy 0.69458336\n",
      "epoch 472 : loss 6.6035247 ; accuracy 0.69493335\n",
      "epoch 473 : loss 6.5942216 ; accuracy 0.69528335\n",
      "epoch 474 : loss 6.5849533 ; accuracy 0.69551665\n",
      "epoch 475 : loss 6.575719 ; accuracy 0.6958\n",
      "epoch 476 : loss 6.5665174 ; accuracy 0.6961167\n",
      "epoch 477 : loss 6.55735 ; accuracy 0.6963\n",
      "epoch 478 : loss 6.548214 ; accuracy 0.69666666\n",
      "epoch 479 : loss 6.5391135 ; accuracy 0.69696665\n",
      "epoch 480 : loss 6.5300517 ; accuracy 0.6972\n",
      "epoch 481 : loss 6.5210247 ; accuracy 0.69748336\n",
      "epoch 482 : loss 6.5120306 ; accuracy 0.69773334\n",
      "epoch 483 : loss 6.5030704 ; accuracy 0.698\n",
      "epoch 484 : loss 6.494145 ; accuracy 0.6982667\n",
      "epoch 485 : loss 6.4852543 ; accuracy 0.69846666\n",
      "epoch 486 : loss 6.4763985 ; accuracy 0.69875\n",
      "epoch 487 : loss 6.4675736 ; accuracy 0.69905\n",
      "epoch 488 : loss 6.458782 ; accuracy 0.6992667\n",
      "epoch 489 : loss 6.450027 ; accuracy 0.6995\n",
      "epoch 490 : loss 6.441305 ; accuracy 0.69965\n",
      "epoch 491 : loss 6.432618 ; accuracy 0.69995\n",
      "epoch 492 : loss 6.4239635 ; accuracy 0.70025\n",
      "epoch 493 : loss 6.415343 ; accuracy 0.70055\n",
      "epoch 494 : loss 6.4067535 ; accuracy 0.7007833\n",
      "epoch 495 : loss 6.3981957 ; accuracy 0.70101666\n",
      "epoch 496 : loss 6.389673 ; accuracy 0.7012333\n",
      "epoch 497 : loss 6.3811827 ; accuracy 0.7015167\n",
      "epoch 498 : loss 6.372724 ; accuracy 0.70176667\n",
      "epoch 499 : loss 6.364299 ; accuracy 0.70203334\n",
      "test loss 6.059619 ; accuracy 0.7051\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = mnist_dataset()\n",
    "for epoch in range(500):\n",
    "    loss, accuracy = train_one_step(model, optimizer, \n",
    "                                    tf.constant(train_data[0], dtype=tf.float32), \n",
    "                                    tf.constant(train_data[1], dtype=tf.int64))\n",
    "    print('epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n",
    "loss, accuracy = test(model, \n",
    "                      tf.constant(test_data[0], dtype=tf.float32), \n",
    "                      tf.constant(test_data[1], dtype=tf.int64))\n",
    "\n",
    "print('test loss', loss.numpy(), '; accuracy', accuracy.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
